{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "defined-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,concatenate_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "absolute-channels",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_column = [\"tokens\",\"ner_tags\"]\n",
    "split_list = [\"train\",\"validation\",\"test\"]\n",
    "\n",
    "def remove_columns_from_dataset_dict(dataset_dict,feature_columns):\n",
    "    assert sorted(split_list) == sorted(list(dataset_dict.keys())), \"Dataset is not containing all splits for train,test,val\"\n",
    "    for split in split_list:\n",
    "        remove_column_list = [col for col in list(dataset_dict[split].features) if col not in feature_column ]\n",
    "        dataset_dict[split] = dataset_dict[split].remove_columns(remove_column_list)\n",
    "    return dataset_dict\n",
    "\n",
    "\n",
    "def merging_all_splits_from_dataset_dict(dataset1,dataset2):\n",
    "    for split in split_list:\n",
    "        assert dataset1[split].features.type == dataset2[split].features.type\n",
    "        dataset1[split] = concatenate_datasets([dataset1[split],dataset2[split]])\n",
    "    return dataset1    \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-decrease",
   "metadata": {},
   "source": [
    "# Preprocessing `wikiann`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikiann= load_dataset(\"wikiann\",\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "prospective-breath",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_selected_validation_wikiann = wikiann[\"validation\"].train_test_split(test_size=0.5)\n",
    "additional_selected_test_wikiann = wikiann[\"test\"].train_test_split(test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "amazing-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert wikiann[\"train\"].features.type == additional_selected_validation_wikiann[\"train\"].features.type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aggressive-cosmetic",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wikiann[\"train\"] = concatenate_datasets([additional_selected_test_wikiann[\"train\"],wikiann[\"train\"]])\n",
    "wikiann[\"validation\"] = additional_selected_validation_wikiann[\"test\"]\n",
    "wikiann[\"test\"] = additional_selected_test_wikiann[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "blocked-universe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wikiann_cleaned = remove_columns_from_dataset_dict(wikiann,feature_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "immune-saturday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikiann_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-holder",
   "metadata": {},
   "source": [
    "# Preprocessing `conll2003`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "metropolitan-elimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/Users/philipp/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"
     ]
    }
   ],
   "source": [
    "conll = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fancy-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_cleaned = remove_columns_from_dataset_dict(conll,feature_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "unique-teens",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-housing",
   "metadata": {},
   "source": [
    "# Merging the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "reliable-frequency",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "External features info don't match the dataset:\nGot\n{'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos_tags': Sequence(feature=ClassLabel(num_classes=47, names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], names_file=None, id=None), length=-1, id=None), 'chunk_tags': Sequence(feature=ClassLabel(num_classes=23, names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], names_file=None, id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)}\nwith type\nstruct<chunk_tags: list<item: int64>, id: string, ner_tags: list<item: int64>, pos_tags: list<item: int64>, tokens: list<item: string>>\n\nbut expected something like\n{'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\nwith type\nstruct<ner_tags: list<item: int64>, tokens: list<item: string>>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-80758341ab71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmerged_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerging_all_splits_from_dataset_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconll_cleaned\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwikiann_cleaned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-93-026545378872>\u001b[0m in \u001b[0;36mmerging_all_splits_from_dataset_dict\u001b[0;34m(dataset1, dataset2)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msplit_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mdataset1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdataset2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mdataset1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdataset1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mconcatenate_datasets\u001b[0;34m(dsets, info, split)\u001b[0m\n\u001b[1;32m   2744\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2745\u001b[0m     \u001b[0;31m# Make final concatenated dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2746\u001b[0;31m     concatenated_dataset = Dataset(\n\u001b[0m\u001b[1;32m   2747\u001b[0m         \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2748\u001b[0m         \u001b[0minfo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hf/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arrow_table, info, split, indices_table, fingerprint)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fingerprint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fingerprint can't be None in a Dataset object\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    258\u001b[0m                 \"External features info don't match the dataset:\\nGot\\n{}\\nwith type\\n{}\\n\\nbut expected something like\\n{}\\nwith type\\n{}\".format(\n\u001b[1;32m    259\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: External features info don't match the dataset:\nGot\n{'id': Value(dtype='string', id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'pos_tags': Sequence(feature=ClassLabel(num_classes=47, names=['\"', \"''\", '#', '$', '(', ')', ',', '.', ':', '``', 'CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNP', 'NNPS', 'NNS', 'NN|SYM', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB'], names_file=None, id=None), length=-1, id=None), 'chunk_tags': Sequence(feature=ClassLabel(num_classes=23, names=['O', 'B-ADJP', 'I-ADJP', 'B-ADVP', 'I-ADVP', 'B-CONJP', 'I-CONJP', 'B-INTJ', 'I-INTJ', 'B-LST', 'I-LST', 'B-NP', 'I-NP', 'B-PP', 'I-PP', 'B-PRT', 'I-PRT', 'B-SBAR', 'I-SBAR', 'B-UCP', 'I-UCP', 'B-VP', 'I-VP'], names_file=None, id=None), length=-1, id=None), 'ner_tags': Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], names_file=None, id=None), length=-1, id=None)}\nwith type\nstruct<chunk_tags: list<item: int64>, id: string, ner_tags: list<item: int64>, pos_tags: list<item: int64>, tokens: list<item: string>>\n\nbut expected something like\n{'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\nwith type\nstruct<ner_tags: list<item: int64>, tokens: list<item: string>>"
     ]
    }
   ],
   "source": [
    "merged_dataset = merging_all_splits_from_dataset_dict(conll_cleaned,wikiann_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-broadcast",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
