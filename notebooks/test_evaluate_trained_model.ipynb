{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "russian-controversy",
   "metadata": {},
   "source": [
    "# Evaluate/Test Trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seqeval torch>=1.6.0 transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hairy-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent = os.path.dirname(os.getcwd())\n",
    "\n",
    "sys.path.insert(0,f'{parent}/src/training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "centered-mercy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import main,parse_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hollywood-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "default_args = parse_args()\n",
    "\n",
    "test_args = {\n",
    "    'model_name_or_path' : 'elastic/distilbert-base-uncased-finetuned-conll03-english',\n",
    "    'dataset':'conll2003',\n",
    "    'task':'ner',\n",
    "    'do_test':True,\n",
    "    'fp16':False,\n",
    "    'output_dir':'.'\n",
    "}\n",
    "\n",
    "\n",
    "merged_args = {**vars(default_args),**test_args}\n",
    "\n",
    "args = Namespace(**merged_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "designed-understanding",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:710] 2021-05-22 18:30:52,955 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:615] 2021-05-22 18:30:53,066 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/22/2021 18:30:53 - WARNING - train -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
      "05/22/2021 18:30:53 - INFO - train -   Training/evaluation parameters TrainingArguments(output_dir=., overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, evaluation_strategy=IntervalStrategy.NO, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, lr_scheduler_type=SchedulerType.LINEAR, warmup_ratio=0.0, warmup_steps=0, logging_dir=./logs, logging_strategy=IntervalStrategy.STEPS, logging_first_step=False, logging_steps=500, save_strategy=IntervalStrategy.STEPS, save_steps=2000, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level=O1, fp16_backend=auto, fp16_full_eval=False, local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name=., disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model=f1, greater_is_better=True, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name=length, report_to=['tensorboard'], ddp_find_unused_parameters=None, dataloader_pin_memory=True, skip_memory_metrics=False, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, _n_gpu=0, mp_parameters=)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:517] 2021-05-22 18:30:53,984 >> loading configuration file https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english/resolve/main/config.json from cache at /Users/philipp/.cache/huggingface/transformers/5d7cdb47cd526d2840b2e6be7aa216676ce5d1bb0637d02c61e3093f21215718.1d93e4fa85540aff79e50ad5b2dff6e2eca88a7505c1f0da1ec573ee47131798\n",
      "[INFO|configuration_utils.py:553] 2021-05-22 18:30:54,009 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PER\",\n",
      "    \"2\": \"I-PER\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"I-ORG\",\n",
      "    \"5\": \"B-LOC\",\n",
      "    \"6\": \"I-LOC\",\n",
      "    \"7\": \"B-MISC\",\n",
      "    \"8\": \"I-MISC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 5,\n",
      "    \"B-MISC\": 7,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-PER\": 1,\n",
      "    \"I-LOC\": 6,\n",
      "    \"I-MISC\": 8,\n",
      "    \"I-ORG\": 4,\n",
      "    \"I-PER\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-22 18:30:56,278 >> loading file https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english/resolve/main/vocab.txt from cache at /Users/philipp/.cache/huggingface/transformers/055ac6c3334d028b4a6ae40f1316d0784a5256e95a45e88c0d2f34f311d0beb8.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-22 18:30:56,302 >> loading file https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english/resolve/main/tokenizer.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-22 18:30:56,333 >> loading file https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english/resolve/main/added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-22 18:30:56,397 >> loading file https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english/resolve/main/special_tokens_map.json from cache at /Users/philipp/.cache/huggingface/transformers/ddc50652d16c45ca779f91f2778bb16c8129bcbd3a17ac465f0670351ebea8de.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\n",
      "[INFO|tokenization_utils_base.py:1717] 2021-05-22 18:30:56,462 >> loading file https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english/resolve/main/tokenizer_config.json from cache at /Users/philipp/.cache/huggingface/transformers/c05a3cc1fed3c13910743ee1c1e1b0202c5ff7ccec021472d59b0ea07d1436c4.fa84c2ee272a04cea269d4e8ed68860995814f9704a2cb5dc117f068631bf21e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/22/2021 18:30:58 - WARNING - datasets.builder -   Reusing dataset conll2003 (/Users/philipp/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n",
      "05/22/2021 18:30:58 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/philipp/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/cache-3424ac310a8ce389.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07985206d9a342b18ebed007b2cc5c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "05/22/2021 18:30:59 - WARNING - datasets.arrow_dataset -   Loading cached processed dataset at /Users/philipp/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6/cache-320586bc8f1f09ec.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:517] 2021-05-22 18:31:01,291 >> loading configuration file https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english/resolve/main/config.json from cache at /Users/philipp/.cache/huggingface/transformers/5d7cdb47cd526d2840b2e6be7aa216676ce5d1bb0637d02c61e3093f21215718.1d93e4fa85540aff79e50ad5b2dff6e2eca88a7505c1f0da1ec573ee47131798\n",
      "[INFO|configuration_utils.py:553] 2021-05-22 18:31:01,315 >> Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"ner\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"O\",\n",
      "    \"1\": \"B-PER\",\n",
      "    \"2\": \"I-PER\",\n",
      "    \"3\": \"B-ORG\",\n",
      "    \"4\": \"I-ORG\",\n",
      "    \"5\": \"B-LOC\",\n",
      "    \"6\": \"I-LOC\",\n",
      "    \"7\": \"B-MISC\",\n",
      "    \"8\": \"I-MISC\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"B-LOC\": 5,\n",
      "    \"B-MISC\": 7,\n",
      "    \"B-ORG\": 3,\n",
      "    \"B-PER\": 1,\n",
      "    \"I-LOC\": 6,\n",
      "    \"I-MISC\": 8,\n",
      "    \"I-ORG\": 4,\n",
      "    \"I-PER\": 2,\n",
      "    \"O\": 0\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.7.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:1155] 2021-05-22 18:31:01,876 >> loading weights file https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english/resolve/main/pytorch_model.bin from cache at /Users/philipp/.cache/huggingface/transformers/4c540f013e47033eae813ac94a5998a7e5c39e1f9cac024a4d9cc95bda172e45.1179cf93314bce5e3bb208f9cb6b8dc0fe5ea94bbda436cbb2da3860a273c504\n",
      "[INFO|modeling_utils.py:1339] 2021-05-22 18:31:03,763 >> All model checkpoint weights were used when initializing DistilBertForTokenClassification.\n",
      "\n",
      "[INFO|modeling_utils.py:1347] 2021-05-22 18:31:03,780 >> All the weights of DistilBertForTokenClassification were initialized from the model checkpoint at elastic/distilbert-base-uncased-finetuned-conll03-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForTokenClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "05/22/2021 18:31:05 - INFO - train -   *** Test ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:515] 2021-05-22 18:31:05,263 >> The following columns in the test set  don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: ner_tags, chunk_tags, pos_tags, tokens, id.\n",
      "[INFO|trainer.py:2115] 2021-05-22 18:31:05,499 >> ***** Running Prediction *****\n",
      "[INFO|trainer.py:2117] 2021-05-22 18:31:05,651 >>   Num examples = 3453\n",
      "[INFO|trainer.py:2120] 2021-05-22 18:31:05,814 >>   Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='432' max='432' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [432/432 18:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer_pt_utils.py:907] 2021-05-22 18:50:11,512 >> ***** test metrics *****\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:11,536 >>   init_mem_cpu_alloc_delta  =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:11,582 >>   init_mem_cpu_peaked_delta =        0MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:11,628 >>   test_accuracy             =     0.9754\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:11,701 >>   test_f1                   =     0.8954\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:11,762 >>   test_loss                 =     0.1404\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:11,835 >>   test_mem_cpu_alloc_delta  =      -32MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:11,914 >>   test_mem_cpu_peaked_delta =       89MB\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:11,942 >>   test_precision            =     0.8936\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:12,016 >>   test_recall               =     0.8972\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:12,050 >>   test_runtime              = 0:19:05.86\n",
      "[INFO|trainer_pt_utils.py:912] 2021-05-22 18:50:12,126 >>   test_samples_per_second   =      3.013\n"
     ]
    }
   ],
   "source": [
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-fraud",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
